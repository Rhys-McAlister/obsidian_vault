A quote taken from Petar Veličković's excellent presentation [Theoretical Foundations of Graph Neural Networks](https://www.youtube.com/watch?v=uF53xsT7mjc&t=3902s&ab_channel=PetarVeli%C4%8Dkovi%C4%87) stated that "Chemistry disrupts ML, not the other way around" which serves as an excellent reminder that many of the key developments in the area of graph neural networks (GNNs) have been developed almost concurrently in the field of computational chemistry. This seems intuitive  as molecules are naturally modelled as graphs and such we can see many examples of GNN like models originating as early as the 90s. 

[A Neural Device for Searching Direct Correlations between Structures and Properties of Chemical Compounds | Journal of Chemical Information and Modeling (acs.org)](https://pubs.acs.org/doi/full/10.1021/ci940128y)
[ChemNet: A Novel Neural Network Based Method for Graph/Property Mapping | Journal of Chemical Information and Modeling (acs.org)](https://pubs.acs.org/doi/10.1021/ci00024a001)

Igor Baskin and coauthors proposed a "neural device" (what we would call a neural network today) that was designed for the purposes of searching for correlations between structures and properties of various organic molecules with preexisting computation of any molecular descriptors.  Similarly, Dmitry Kireev published his ChemNet paper in 1995 in which sets of graph distance matrices were used in place of traditional molecular descriptors for the application of property mapping. These papers demonstrate a key moment that has occurred concurrently in many domains when the creator of the model steps back from hand crafting and allows the model to determine key features of a given problem. This paradigm shift falls inline with a famous article by Rich Sutton called the [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)in which the author proposes that the biggest lesson that we have gained from over 70 years of AI research is that methods that leverage computation are going to ultimately be the most successful and by a significant margin at that. Sutton also proposes that although methods that leverage computation and methods that leverage domain knowledge aren't necessarily mutually exclusive, time spend on one does detract from time spent on another and in the long run the methods focused on computation tend to outcompete the alternatives. In the context of chemistry we can draw a comparison here to the work of Baskin and coauthors where the task of feature generation has been abstracted away to our models in place of hand crafted features. For those with some familiarity with the progress of Deep learning, this closely follows the progress from hand crafted computer vision features to convolution based approaches.

Returning to the quote taken from Petar's presentation, we can see a paper published from Merkwirth and Lenguaer that proposed many of the key elements found in message passing frameworks today as early as 2005. This paper describes a method for the automatic generation of features for molecular datasets. This is done via the transformation of a molecular graph into a graph level molecular descriptor. They propose their system closely in line with what we see today in that their system is trained via gradient descent to optimise the molecular embedding in order to generate accurate predictions of graph level molecular properties. 



