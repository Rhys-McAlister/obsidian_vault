**[[Graph convolutions]]**

- Convolutions in the context of images refers to the process of sliding a convolutional filter over an image and at each step a weighted sum is calculated between the filter and the portion of the image it is currently operating over (the receptive field).

- We can consider filters to serve as a mechanism for detecting specific features. This approach of using filters for feature recognition is well suited for image data for the following reasons. Shift in-variance, the ability to recognise a feature regardless of its position within an image is key and is afforded by this approach. Assumption of locality, in that nearby pixels can be considered to be closely related. Hierarchy, larger portions of an image can be decomposed into combinations of various associated smaller components.

- Similarly to images, we can consider several priors which suggest that a convolutional approach is suitable for this data structure. Similarly to images, graphs have a locality prior in that we assume that nodes closer together are more relevant than those further away. The manner in which we define this is distinct however in that in images locality is defined by position in two-dimensional space where as locality in graphs is a structural notion (number of 'hops' away).

- A further prior on graph data is permutation in-variance, this means that the manner in which we order our nodes should not affect the output. This is an important consideration as since a given graph can be represented by multiple adjacency matrices, any graph convolution performed should be permutation invariant.

- A convolutional approach also has the benefit of being able to function with a fixed parameter set over graphs of variable sizes. This is key in graph based problems as graph datasets are often variable in size.

- We interpret each row of $X$ to be an embedding of the information stored at a given node corresponding to that row. A graph convolution will then update that embedding at each node based upon this embedding and the embedding of the neighbours of a given node.

A simple graph convolution can take the following form:

$$x_i^{'} = x_iW_I + \sum_{j \in N(i)} x_jW_2 + b$$

In this example $x_i^{'}$ represents an updated embedding for a given node $i$. $W_1$ and $W_2$ represent two matrices of learning weights for our filters and $b$ is a learnable bias vector.

