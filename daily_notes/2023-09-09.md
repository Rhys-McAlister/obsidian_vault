[[Activation functions]]

The [[Logistic function]] is a special instance of the [[sigmoid function]].

The [[Logistic function]] can be used to model the probability that a given sample $x$ belongs to a positive class within the context of a binary classification task.

$$ z = \Sigma^m_{i = 0} w_i x_i = W^Tx$$

[[Softmax]] in contrast to the argmax function returns the probability of each class as opposed to its index. This allows us to compute class probabilities in the context of multiclass problems (multinomial logistic regression).

The [[Softmax]]  function calculates the probability that a given sample with net input $z$ belonging to the ith class using a normalization term in its denominator. 

Rectified linear unit ([[ReLU]]) is another activation function often used in training neural networks.

[[ReLU]] solves the [[Vanishing gradient problem]] that comes from using tanh and logistic activations. 

[[Xavier-Glorot initialization]]

Optimising NNs requires the gradients of the loss with respect to the parameter weights such that optimisation algorithms like SGD and adam can function. However, these gradients also have other uses, such as being able to examine the network to discover why our model is making a particular prediction for a test example.

[[Automatic differentiation]] can be thought of as the use of the chain rule for calculating gradients of nested functions. 