## Implementing a Multilayer NN from Scratch

What happened if you add too many layers to your deep learning model?
- Adding additional hidden layers to an MLP model creates a deeper network architecture and this decision to add additional layers is essentially a hyper parameter that we can optimise for the context of the problem we are considering. It should be noted however that with additional layers added the loss gradients that are used for updating the network's parameters will become increasingly small. This issue is defined as the [[Vanishing gradient problem]] and creates difficulties for the models learning.

The process of [[Forward propagation]] is used to calculate the output of an MLP model. 

The [[MLP]] learning procedure runs as follows:
1. Beginning at the input layer, the patterns of the training data are propagated through the network in order to generate an output.
2. From the output of our network we can calculate a loss value.
3. From the loss value, we back propagate to find its derivative with respect to each of our weight and bias parameters within our network and update the model.

Following this procedure for multiple epochs allows for the learning of the weight and bias parameters of our model. From these learned parameters we can use our forward propagation function to calculate the output from our network, apply a threshold function to this output in order to yield predicted class labels. 